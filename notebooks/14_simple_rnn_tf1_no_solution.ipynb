{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Simple RNN TF1\n",
    "\n",
    "** This is the implementation for TensorFlow 1.0 and python 3.4. It uses the Keras or the TF-RNN library for training.** For another more manual implementation see simple_rnn in dl_tutorial\n",
    "\n",
    "In this notebook we consider a simple example of an RNN and used a quite artifical data generating process (if you have a better idea / story please contact me). \n",
    "\n",
    "The example has been motivated by:\n",
    "http://r2rt.com/recurrent-neural-networks-in-tensorflow-i.html. \n",
    "\n",
    "Other Resources for RNNs:\n",
    "\n",
    "* http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/\n",
    "* http://r2rt.com/recurrent-neural-networks-in-tensorflow-i.html\n",
    "* http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "* http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "* http://www.deeplearningbook.org/contents/rnn.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from six.moves.cPickle import loads\n",
    "import numpy as np\n",
    "import sys\n",
    "np.random.seed(42)\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(1)\n",
    "\n",
    "import keras\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "tf.__version__, sys.version_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Global config variables (see below)\n",
    "num_steps = 40     # number of truncated backprop steps\n",
    "batch_size = 200  # number of minibatches b\n",
    "num_classes_in = 3   # number of classes in the input\n",
    "num_classes_out = 2   # number of classes in the output\n",
    "state_size = 4    # number of classes in the state\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Helper functions\n",
    "def one_hot(Y, max):\n",
    "    d = np.zeros((len(Y),max), dtype='int32')\n",
    "    for row,col in enumerate(Y):\n",
    "        d[row, col] = 1\n",
    "    return d    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Definition of the task\n",
    "\n",
    "We consider a network which predicts at each point in time a variable $\\hat{y}_t$ based on earlier values of $\\hat{y}_{t'}$ covariates $x_t$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Example data  (I screama, you screama, we all screama for I screama)\n",
    "\n",
    "We need some data to play around with RNNs. They are capable of doing quite complicated things such as language models and so on. For this example, we want to generate the data ourself. We have to come up with a process which creates $x_t$ which itself can be influcenced by events $x_{t'}$ which happend before $t$. Further, we have to come up with $y_t$ which depends on $x_t'$ for timepoints $t' \\le t$. \n",
    "\n",
    "To keep it simple, we analyse the following quite artifical process in which the weather $x_{t'}$ for $t' \\le t$ influences our stock on icecream $y_t$. We then see if the RNN is capable of reconstructing that process.\n",
    "\n",
    "#### Definition of the simple process\n",
    "The weather $x_t$ at a certain point in time $t$ has three states (sunny, rainy, cloudy), which we model as $x_t = (1,0,0)$, $x_t = (0,1,0)$, and $x_t = (0,0,1)$ repectively. We assume that the weather is completly random (of course we could model more complex scenarios). \n",
    "\n",
    "We have an icecream store capable of holding 2 units of icecream and we start with a full store. When it is sunny we sell one unit of icecream. We have the strange policy that we order  on unit of icecream when it's cloudy. It takes 3 days to deliever the ice cream, we accept the ice cream if we do not have a full stock.\n",
    "\n",
    "This enables us to model $y_t$ the state of the store $(1,0)$ for out of stock and $(0,1)$ for in stock. We create the one-hot-encoded data in the graph later. For now we use integers but keep in mind that the data is categorical. \n",
    "\n",
    "** The important part is that, we have values $y_t$ which can be prediced from earlier** $x_t$s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def gen_data(size=1000000):\n",
    "    Xs = np.array(np.random.choice(3, size=(size,))) #Random Weather\n",
    "    Y = []\n",
    "    ice = 2 #Our stock of icecream at start\n",
    "    for t,x in enumerate(Xs):\n",
    "        # (t-3) >= 0 the first ice cream could be delivered on day 3\n",
    "        # Xs[t - 3] claudy three days before today => we ordered ice cream\n",
    "        # ice < 2 not full\n",
    "        if (t - 3) >= 0 and Xs[t - 3] == 1 and ice < 2: \n",
    "            ice += 1\n",
    "        if x == 0: # It is sunny we therefore sell ice, if we have\n",
    "            if ice > 0: # We have ice cream\n",
    "                ice -= 1\n",
    "        if ice > 0: #We are not out of stock\n",
    "            Y.append(1)\n",
    "        else:\n",
    "            Y.append(0)\n",
    "    return Xs, np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train, Y_train = gen_data(50000) #Global variables holding the input and output\n",
    "(X_train[0:50], Y_train[0:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Forward pass in numpy\n",
    "To better illustrate the used method, we first do a forward-pass of the RNN using numpy. We load the weights which we calculated previously with the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "W_, b_, V_, bv_ = np.load('14_rnn_weights_tf1.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Architecture of the network \n",
    "We now define the network, we do not consider the output nodes yet.\n",
    "A single RNN cell is shown in the figure below in the middle:\n",
    "\n",
    "![](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png)\n",
    "Image taken from: [Colah's RNN Blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "\n",
    "The joining of the two lines coming from the previous state $h_{t-1}$ and the current x-values $x_t$ is a concantination to a vector  $[h_{t-1}, x_{t}]$ of size `state_size + num_classes_in`. Alternatively, instead of concatinating, one could also use two matrices $W_x$ and $W_h$ and keep the states seperate. This is mathematically completely identical. The new state $h_t$ is then calculated as:\n",
    "\n",
    "$$\n",
    "    h_{t} = \\tanh([h_{t-1}, x_{t}] \\cdot W + b) = \\tanh(h_{t-1} \\cdot W_h + x_{t} \\cdot U + b)\n",
    "$$\n",
    "\n",
    "The dynamic of the hidden state $h_{t}$ is determined by $W$ (and $b$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(\"W = \",W_)\n",
    "print(\"b = \",b_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# The first state\n",
    "h0 = np.zeros(state_size) #We start with 0 initial state\n",
    "x1 = one_hot(X_train, num_classes_in)[0] #Make a vector\n",
    "\n",
    "# hints: use np.tanh(array) for the tangens hyperbolicus function\n",
    "#        use np.concatenate([array1,array2]) or np.hstack([array1,Array2]) to bind two arrays to together     \n",
    "#        use np.matmul(array1,array2) for matrixmultiplication   \n",
    "\n",
    "# important: tensorflow binds first the U-matrix and then the Wh-matrix to the W-matrix,\n",
    "#            thats why we have to bind fist xt and then ht-1 together and then multiply it with the W-matrix\n",
    "\n",
    "#<---- your code here (calculate the hidden state h1) ---->\n",
    "\n",
    "\n",
    "#<---- end your code here ---->\n",
    "\n",
    "print(h0, \"--->\", h1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We could repeat those transitions of the hidden states to get a sequence of hidden states:\n",
    "\n",
    "$h_0 \\rightarrow h_1 \\rightarrow h_2 \\rightarrow h_3 \\rightarrow h_4 \\ldots $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def rnn_forward(state, X_train):\n",
    "    hs = []\n",
    "    for t in range(len(X_train)):\n",
    "        # Note that TF concatenates [Input, State]\n",
    "        state = np.tanh(np.matmul(np.concatenate([X_train[t,:],state]), W_) + b_)\n",
    "        hs.append(state)\n",
    "    return hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rnn_forward(h0, one_hot(X_train[0:5],num_classes_in))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We add some output. For each time step the output is produced by multiplying the hidden state with:\n",
    "\n",
    "$o_t = h_t \\cdot V + b_{\\tt{v}}$\n",
    "\n",
    "This is a logit, the final the probability of output class is the softmax of the logit.\n",
    "\n",
    "reminder:\n",
    "$softmax(x)_{i}=\\frac{e^{x_i}}{\\sum_{} e^{x_i}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# hints: use np.matmul(array1,array2) for matrixmultiplication \n",
    "#        use np.exp(array) and np.sum(array)for the softmax function\n",
    "\n",
    "#<---- your code here (calculate the output state o1 for timestep 1 from h1, V_ and the bias bv_) ---->\n",
    "\n",
    "\n",
    "\n",
    "#<---- your code here (calculate probability from the state o1) using softmax ---->\n",
    "\n",
    "\n",
    "#<---- end your code here  ---->\n",
    "o1, prob_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "h = rnn_forward(h0, one_hot(X_train,3))\n",
    "pt = []\n",
    "for t in range(len(h)):\n",
    "    ot = np.matmul(h[t], V_) + bv_\n",
    "    pt.append(np.exp(ot)/np.sum(np.exp(ot)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pt[0:10], np.argmax(pt[0:30],axis=1), Y_train[0:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#accuracy\n",
    "np.average(np.argmax(pt, axis=1) == Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tot_loss = 0\n",
    "for i in range(len(Y_train)):\n",
    "    tot_loss += -np.log(pt[i][Y_train[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tot_loss / len(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Training of the network\n",
    "### Preparation of the Minibatch\n",
    "\n",
    "In this example, we have in principle a large stream of data $x$ and $y$. For efficiency reason we split the stream in minibatches of a certain length. For this task we could also imagin to have several realizations of that icecream process, so that it would also be natural to split the process into mini batches. \n",
    "\n",
    "For simplicity, we create the minibatch by randomly cutting out `batch_size` entries of fixed length `num_steps`. Other, more advanced ways of doing so are possible. See e.g. https://danijar.com/variable-sequence-lengths-in-tensorflow/. For the time being, we thus consider the input tensor $X_{btc}$ for the minibatch to be of the following form:\n",
    "\n",
    "* $b$ having `batch_size` entries\n",
    "* $t$ loops over the unrolled timestamps (`num_steps`)\n",
    "* $c$ has the dimension of the one-hot-coded input classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def data_matrix(Xs, Ys, size = 32, num_steps = 50):\n",
    "    data_x = np.zeros([size, num_steps, 3], dtype=np.int32)\n",
    "    data_y = np.zeros([size, num_steps, 2], dtype=np.int32)\n",
    "    for i in range(1,size):\n",
    "        s = int(np.random.uniform(0, len(Xs) - num_steps))\n",
    "        data_x[i] = one_hot(Xs[s : s + num_steps],3)\n",
    "        data_y[i] = one_hot(Ys[s : s + num_steps],2)\n",
    "    return data_x, data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X, Y = data_matrix(X_train, Y_train, size=10000, num_steps=num_steps)\n",
    "X.shape, Y.shape\n",
    "#print (X[0:2,0:5])\n",
    "#print (Y[0:2,0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Building the model with keras\n",
    "\n",
    "**Keras assumes (batch, time, input_dimension)** as input tensor. The batch dimension needs not to be specified (side note that specifying it to `None` would not work).\n",
    "\n",
    "\n",
    "In our model we calculate the loss using all hidden timepoints. This is many-to-many situation is different for example to a sentiment classifier, where we we have a many-to-one situation. \n",
    "\n",
    "Not just the latest one (in time). Hence we set `return_sequences=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "rnn = keras.layers.SimpleRNN(state_size, input_shape=(num_steps, 3), return_sequences=True, name='RNN')\n",
    "model.add(rnn) \n",
    "#model.input_shape --> (None, 40, 3)\n",
    "#model.output_shape --> (None, 40, 4) \n",
    "\n",
    "# Add an output layer connecting with the hidden state of the RNN (use the TimeDistributed Layer)\n",
    "#model.add(keras.layers.Dense(2))\n",
    "model.add(keras.layers.TimeDistributed(keras.layers.Dense(2)))\n",
    "#model.output_shape --> (None, 40, 2) \n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model.fit(X, Y, epochs=100, verbose=2, validation_split=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# We use the training, since we did use only a random subset of the data\n",
    "X, Y = data_matrix(X_train, Y_train, size=1000, num_steps=num_steps)\n",
    "model.evaluate(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rnn.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Using the TensorFlow API\n",
    "\n",
    "Alternatively one can use the TensorFlow-API for creating RNNs. In principle there are two TensorFlow methods. The first, kind of deprecated one, builds a graph from the unrolled network. This API has issues in performance, first of all the creation of the graph takes quite some time. Further, and this is a bit it is also slower during runtime. Therefore, the novel dynamic API should be prefered. If you want to use sequences of variable length see:  https://danijar.com/variable-sequence-lengths-in-tensorflow/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_batch(Xs, Ys, batch_size = 32, num_steps = 50):\n",
    "    data_x = np.zeros([batch_size, num_steps], dtype=np.int32)\n",
    "    data_y = np.zeros([batch_size, num_steps], dtype=np.int32)\n",
    "    for i in range(1,batch_size):\n",
    "        s = int(np.random.uniform(0, len(Xs) - num_steps))\n",
    "        data_x[i] = Xs[s : s + num_steps]\n",
    "        data_y[i] = Ys[s : s + num_steps]  \n",
    "    return data_x, data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(42)\n",
    "# Placeholders\n",
    "x = tf.placeholder(tf.int32, [batch_size, num_steps], name='input_placeholder')\n",
    "y = tf.placeholder(tf.int32, [batch_size, num_steps], name='labels_placeholder')\n",
    "init_state = tf.zeros([batch_size, state_size])\n",
    "\n",
    "# RNN Inputs\n",
    "# One hot encoding.\n",
    "x_one_hot = tf.one_hot(x, num_classes_in)\n",
    "# We want the following dimensions [batch_size, Max_Length, num_classes_in]\n",
    "rnn_inputs = tf.transpose(x_one_hot, perm=(0,1,2))\n",
    "rnn_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Definition of the basic cell\n",
    "\n",
    "We have to define the elementary cell, which has a state of a given size. As above we use the basic RNN-Cell which is described in: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#cell = tf.nn.rnn_cell.BasicRNNCell(state_size) \n",
    "#cell = tf.nn.rnn_cell.BasicLSTMCell(state_size)\n",
    "# For tf1.0 the cells have been temporarily moved to a contib see: https://github.com/tensorflow/models/issues/919\n",
    "cell = tf.contrib.rnn.BasicRNNCell(state_size)\n",
    "init_state = cell.zero_state(batch_size, tf.float32)\n",
    "rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, initial_state=init_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rnn_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The output $o_{btk}$ tensor produces for each minibatch and timepoint a 4 (number of output states) dimensional vector indexed by $k$. For each timepoint and batch, we later want to compare this with the corresponding y-value with has the shape $y_{bt}$. In a first step we flatten the b and t dimension to a $200*40 = 8000$ dimensional vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#reshape rnn_outputs and y so we can get the logits in a single matmul\n",
    "rnn_outputs = tf.reshape(rnn_outputs, [-1, state_size])\n",
    "y_reshaped = tf.reshape(y, [-1])\n",
    "y_reshaped, rnn_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('softmax'):\n",
    "    V = tf.get_variable('V', [state_size, num_classes_out])\n",
    "    bv = tf.get_variable('bv', [num_classes_out], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "logits = tf.matmul(rnn_outputs, V) + bv\n",
    "logits, y_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#writer = tf.summary.FileWriter(\"tb_simple_rnn_tf1/dd\", tf.get_default_graph()) \n",
    "#writer.close()\n",
    "#!tensorboard --logdir=tb_simple_rnn_tf1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "total_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_reshaped, logits=logits))\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "Y = None\n",
    "X = None\n",
    "count = 0\n",
    "sum_tr_losses = 0\n",
    "sess = tf.Session()\n",
    "#with tf.Session() as sess:\n",
    "sess.run(tf.initialize_all_variables())\n",
    "for i in range(1000):\n",
    "    X, Y = get_batch(X_train, Y_train, batch_size, num_steps)\n",
    "    tr_losses, _ = sess.run([total_loss, train_step], feed_dict={x:X, y:Y})\n",
    "    count += 1\n",
    "    sum_tr_losses += tr_losses\n",
    "    if (i < 10) or (i % 200 == 0):\n",
    "        print (\"{} {}\".format(i, sum_tr_losses / count))\n",
    "        count = 0\n",
    "        sum_tr_losses = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Test on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X, Y = get_batch(X_train, Y_train, batch_size, num_steps)\n",
    "loss_train = sess.run(total_loss, feed_dict={x:X, y:Y})\n",
    "loss_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Getting the relevant weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Finding the relavant weights\n",
    "#ops = tf.get_default_graph().get_operations()\n",
    "#for i in ops:\n",
    "#   print(i.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "graph = tf.get_default_graph()\n",
    "W = graph.get_tensor_by_name('rnn/basic_rnn_cell/weights:0')\n",
    "b = graph.get_tensor_by_name('rnn/basic_rnn_cell/biases:0')\n",
    "W_,b_,V_,bv_ = sess.run([W,b,V,bv])\n",
    "print (\"W =\",W_)\n",
    "print (\"b =\",b_)\n",
    "print (\"V =\",V_)\n",
    "print (\"bv =\",bv_)\n",
    "np.save('14_rnn_weights_tf1', [W_,b_,V_,bv_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
